{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dee91d9-660d-42b2-9be1-eaef72e680a3",
   "metadata": {},
   "source": [
    "# **Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems**\n",
    "### *Copyright (c) 2025, Rye Stahle-Smith* \n",
    "---\n",
    "#### ***Description:***\n",
    "#### `train_model.py` — Model Training and Export (CPU Only)\n",
    "\n",
    "This script trains a supervised machine learning model to detect malicious FPGA bitstreams using byte-level and structural features.\n",
    "\n",
    "> ⚠️ **Note:** This program must be executed on a general-purpose CPU (e.g., your laptop or workstation).  \n",
    "> It is **not compatible with the PYNQ-Z1 board** due to:\n",
    "> - High computational demands during training  \n",
    "> - No supported `scikit-learn` wheel for the PYNQ-Z1's ARM architecture\n",
    "\n",
    "#### ***Features:***\n",
    "- Byte-level and structural feature extraction from `.bit` files  \n",
    "- Dimensionality reduction via TSVD  \n",
    "- Class balancing with SMOTE  \n",
    "- Training multiple classifiers (e.g., Random Forest, SVM)  \n",
    "- Evaluation using Stratified k-Fold Cross-Validation  \n",
    "- Model and TSVD components exported as a `.tar.gz` archive for use on PYNQ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2ae3b4-db46-4960-b660-abb8ce8c6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Organizing bitstreams... ===\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tarfile\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Collect Bitstreams\n",
    "# This portion of the program organizes the bitstreams respectively.\n",
    "# --------------------------\n",
    "EMPTY_FILES = glob.glob(\"trusthub_bitstreams/Empty/*.bit\")\n",
    "BENIGN_FILES = glob.glob(\"trusthub_bitstreams/Benign/*.bit\")\n",
    "MAL_FILES = glob.glob(\"trusthub_bitstreams/Malicious/*.bit\")\n",
    "ALL_FILES = EMPTY_FILES + BENIGN_FILES + MAL_FILES\n",
    "print(\"=== Organizing bitstreams... ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d14e1cc-de95-4c63-b437-005de45011b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extracting sparse features... ===\n",
      "Progress: |████████████████████| 100% (122/122)\n",
      "=== Defining labels... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 2: Feature Extraction\n",
    "# This portion of the program sets the features and labels for the dataset.\n",
    "# --------------------------\n",
    "def extract_sparse_features(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = f.read()\n",
    "    size = len(data)\n",
    "    if size == 0:\n",
    "        return np.zeros(256)\n",
    "    counts = Counter(data)\n",
    "    dense_vec = np.zeros(256)\n",
    "    for byte_val, count in counts.items():\n",
    "        dense_vec[byte_val] = count / size\n",
    "    return dense_vec\n",
    "\n",
    "def display_progress(current, total):\n",
    "    bar_length = 20\n",
    "    percent = int((current / total) * 100)\n",
    "    blocks = int((current / total) * bar_length)\n",
    "    bar = '█' * blocks + '-' * (bar_length - blocks)\n",
    "    sys.stdout.write(f'\\rProgress: |{bar}| {percent}% ({current}/{total})')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"=== Extracting sparse features... ===\")\n",
    "feature_matrix = []\n",
    "for i, f in enumerate(ALL_FILES, 1):\n",
    "    feature_matrix.append(extract_sparse_features(f))\n",
    "    display_progress(i, len(ALL_FILES))\n",
    "\n",
    "print(\"\\n=== Defining labels... ===\")\n",
    "y = [0]*len(EMPTY_FILES) + [1]*len(BENIGN_FILES) + [2]*len(MAL_FILES)\n",
    "X = np.array(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0118a8e-238d-45d4-a23f-f4eba01f8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Applying Truncated Singular Value Decomposition (TSVD)... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 3: Apply Truncated Singular Value Decomposition (TSVD)\n",
    "# This portion of the program uses TSVD to reduce dataset dimensionality and preserve its most important characteristics.\n",
    "# --------------------------\n",
    "print(\"=== Applying Truncated Singular Value Decomposition (TSVD)... ===\")\n",
    "sparse_X = csr_matrix(X)\n",
    "tsvd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_reduced = tsvd.fit_transform(sparse_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c6df8e-4a19-4103-a6f1-5363878f2fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Splitting the dataset for training/ testing... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 4: Train/Test Split\n",
    "# This portion of the program splits and scales the dataset into a complete training and testing set.\n",
    "# --------------------------\n",
    "print(\"=== Splitting the dataset for training/ testing... ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c88b80-5c3d-45d7-bb6a-0d221a948325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparing k_neighbors values for Synthetic Minority Oversampling Technique (SMOTE)... ===\n",
      "\n",
      "SMOTE (k=2): F1 Macro = 0.9210 ± 0.0198\n",
      "SMOTE (k=5): F1 Macro = 0.9494 ± 0.0650\n",
      "SMOTE (k=7): F1 Macro = 0.9696 ± 0.0352\n",
      "SMOTE (k=9): F1 Macro = 0.9706 ± 0.0417\n",
      "SMOTE (k=11): F1 Macro = 0.9706 ± 0.0483\n",
      "\n",
      "=== Applying SMOTE with k_neighbors=9... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 5: Apply SMOTE on Training Set\n",
    "# This portion of the program uses SMOTE to balance class distributions by generating synthetic samples for minority classes.\n",
    "# Thus, improving model performance on imbalanced datasets.\n",
    "# --------------------------\n",
    "print(\"=== Comparing k_neighbors values for Synthetic Minority Oversampling Technique (SMOTE)... ===\\n\")\n",
    "k_values = [2, 5, 7, 9, 11]\n",
    "smote_results = {}\n",
    "best_k = None\n",
    "best_score = 0\n",
    "\n",
    "for k in k_values:\n",
    "    try:\n",
    "        smote = SMOTE(k_neighbors=k, random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        scores = cross_val_score(model, X_train_smote, y_train_smote, cv=skf, scoring='f1_macro')\n",
    "        mean_score = scores.mean()\n",
    "        \n",
    "        smote_results[k] = mean_score\n",
    "        print(f\"SMOTE (k={k}): F1 Macro = {mean_score:.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"SMOTE (k={k}) failed: {e}\")\n",
    "\n",
    "print(f\"\\n=== Applying SMOTE with k_neighbors={best_k}... ===\")\n",
    "smote = SMOTE(k_neighbors=best_k, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a899c7-4b1e-4c99-9937-c68dda3c6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparing classifiers using Stratified k-Fold Cross-Validation (kFCV)... ===\n",
      "\n",
      "=== k = 3 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.03\n",
      "  Precision: 0.97 ± 0.02\n",
      "  Recall: 0.97 ± 0.02\n",
      "  F1: 0.97 ± 0.02\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.90 ± 0.03\n",
      "  Precision: 0.93 ± 0.02\n",
      "  Recall: 0.91 ± 0.03\n",
      "  F1: 0.91 ± 0.03\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.90 ± 0.03\n",
      "  Precision: 0.93 ± 0.02\n",
      "  Recall: 0.91 ± 0.03\n",
      "  F1: 0.91 ± 0.03\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.37 ± 0.02\n",
      "  Precision: 0.17 ± 0.07\n",
      "  Recall: 0.34 ± 0.01\n",
      "  F1: 0.22 ± 0.06\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.05\n",
      "  Precision: 0.79 ± 0.04\n",
      "  Recall: 0.77 ± 0.04\n",
      "  F1: 0.75 ± 0.04\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.37 ± 0.02\n",
      "  Precision: 0.17 ± 0.07\n",
      "  Recall: 0.34 ± 0.01\n",
      "  F1: 0.22 ± 0.06\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.87 ± 0.03\n",
      "  Precision: 0.89 ± 0.02\n",
      "  Recall: 0.88 ± 0.03\n",
      "  F1: 0.88 ± 0.03\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.85 ± 0.04\n",
      "  Precision: 0.89 ± 0.02\n",
      "  Recall: 0.86 ± 0.04\n",
      "  F1: 0.86 ± 0.04\n",
      "\n",
      "=== k = 4 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.02\n",
      "  Precision: 0.97 ± 0.02\n",
      "  Recall: 0.97 ± 0.02\n",
      "  F1: 0.97 ± 0.02\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.91 ± 0.03\n",
      "  Precision: 0.93 ± 0.02\n",
      "  Recall: 0.92 ± 0.03\n",
      "  F1: 0.92 ± 0.03\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.91 ± 0.03\n",
      "  Precision: 0.93 ± 0.02\n",
      "  Recall: 0.92 ± 0.03\n",
      "  F1: 0.92 ± 0.03\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.35 ± 0.01\n",
      "  Precision: 0.12 ± 0.00\n",
      "  Recall: 0.33 ± 0.00\n",
      "  F1: 0.17 ± 0.00\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.06\n",
      "  Precision: 0.79 ± 0.04\n",
      "  Recall: 0.77 ± 0.05\n",
      "  F1: 0.75 ± 0.06\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.35 ± 0.01\n",
      "  Precision: 0.12 ± 0.00\n",
      "  Recall: 0.33 ± 0.00\n",
      "  F1: 0.17 ± 0.00\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.88 ± 0.04\n",
      "  Precision: 0.90 ± 0.03\n",
      "  Recall: 0.89 ± 0.04\n",
      "  F1: 0.89 ± 0.03\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.90 ± 0.07\n",
      "  Precision: 0.93 ± 0.03\n",
      "  Recall: 0.91 ± 0.07\n",
      "  F1: 0.90 ± 0.08\n",
      "\n",
      "=== k = 5 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.03\n",
      "  Precision: 0.97 ± 0.02\n",
      "  Recall: 0.97 ± 0.02\n",
      "  F1: 0.97 ± 0.02\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.90 ± 0.04\n",
      "  Precision: 0.93 ± 0.03\n",
      "  Recall: 0.91 ± 0.04\n",
      "  F1: 0.91 ± 0.04\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.90 ± 0.04\n",
      "  Precision: 0.93 ± 0.03\n",
      "  Recall: 0.91 ± 0.04\n",
      "  F1: 0.91 ± 0.04\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.34 ± 0.08\n",
      "  Precision: 0.19 ± 0.08\n",
      "  Recall: 0.31 ± 0.06\n",
      "  F1: 0.23 ± 0.08\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.72 ± 0.11\n",
      "  Precision: 0.75 ± 0.15\n",
      "  Recall: 0.76 ± 0.09\n",
      "  F1: 0.73 ± 0.13\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.34 ± 0.08\n",
      "  Precision: 0.19 ± 0.08\n",
      "  Recall: 0.31 ± 0.06\n",
      "  F1: 0.23 ± 0.08\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.91 ± 0.03\n",
      "  Precision: 0.93 ± 0.02\n",
      "  Recall: 0.92 ± 0.02\n",
      "  F1: 0.92 ± 0.02\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.91 ± 0.04\n",
      "  Precision: 0.94 ± 0.02\n",
      "  Recall: 0.92 ± 0.05\n",
      "  F1: 0.92 ± 0.05\n",
      "\n",
      "=== k = 6 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.03\n",
      "  Precision: 0.98 ± 0.02\n",
      "  Recall: 0.97 ± 0.03\n",
      "  F1: 0.97 ± 0.03\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.90 ± 0.06\n",
      "  Precision: 0.93 ± 0.04\n",
      "  Recall: 0.91 ± 0.06\n",
      "  F1: 0.90 ± 0.07\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.90 ± 0.06\n",
      "  Precision: 0.93 ± 0.04\n",
      "  Recall: 0.91 ± 0.06\n",
      "  F1: 0.90 ± 0.07\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.32 ± 0.06\n",
      "  Precision: 0.14 ± 0.06\n",
      "  Recall: 0.31 ± 0.06\n",
      "  F1: 0.18 ± 0.05\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.07\n",
      "  Precision: 0.80 ± 0.06\n",
      "  Recall: 0.77 ± 0.05\n",
      "  F1: 0.75 ± 0.07\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.32 ± 0.06\n",
      "  Precision: 0.14 ± 0.06\n",
      "  Recall: 0.31 ± 0.06\n",
      "  F1: 0.18 ± 0.05\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.91 ± 0.06\n",
      "  Precision: 0.93 ± 0.05\n",
      "  Recall: 0.92 ± 0.06\n",
      "  F1: 0.92 ± 0.06\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.89 ± 0.07\n",
      "  Precision: 0.92 ± 0.04\n",
      "  Recall: 0.90 ± 0.08\n",
      "  F1: 0.89 ± 0.09\n",
      "\n",
      "=== k = 7 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.04\n",
      "  Precision: 0.98 ± 0.03\n",
      "  Recall: 0.97 ± 0.04\n",
      "  F1: 0.97 ± 0.04\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.88 ± 0.08\n",
      "  Precision: 0.93 ± 0.03\n",
      "  Recall: 0.89 ± 0.07\n",
      "  F1: 0.88 ± 0.09\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.88 ± 0.08\n",
      "  Precision: 0.93 ± 0.03\n",
      "  Recall: 0.89 ± 0.07\n",
      "  F1: 0.88 ± 0.09\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.30 ± 0.13\n",
      "  Precision: 0.17 ± 0.10\n",
      "  Recall: 0.28 ± 0.12\n",
      "  F1: 0.20 ± 0.11\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.08\n",
      "  Precision: 0.80 ± 0.09\n",
      "  Recall: 0.76 ± 0.07\n",
      "  F1: 0.75 ± 0.08\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.30 ± 0.13\n",
      "  Precision: 0.17 ± 0.10\n",
      "  Recall: 0.28 ± 0.12\n",
      "  F1: 0.20 ± 0.11\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.91 ± 0.06\n",
      "  Precision: 0.94 ± 0.05\n",
      "  Recall: 0.92 ± 0.06\n",
      "  F1: 0.92 ± 0.06\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.91 ± 0.10\n",
      "  Precision: 0.95 ± 0.05\n",
      "  Recall: 0.92 ± 0.08\n",
      "  F1: 0.92 ± 0.10\n",
      "\n",
      "=== k = 8 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.04\n",
      "  Precision: 0.98 ± 0.03\n",
      "  Recall: 0.97 ± 0.04\n",
      "  F1: 0.97 ± 0.04\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.87 ± 0.10\n",
      "  Precision: 0.87 ± 0.15\n",
      "  Recall: 0.88 ± 0.09\n",
      "  F1: 0.86 ± 0.13\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.87 ± 0.10\n",
      "  Precision: 0.87 ± 0.15\n",
      "  Recall: 0.88 ± 0.09\n",
      "  F1: 0.86 ± 0.13\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.39 ± 0.07\n",
      "  Precision: 0.21 ± 0.10\n",
      "  Recall: 0.36 ± 0.06\n",
      "  F1: 0.25 ± 0.09\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.09\n",
      "  Precision: 0.81 ± 0.09\n",
      "  Recall: 0.76 ± 0.07\n",
      "  F1: 0.75 ± 0.08\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.39 ± 0.07\n",
      "  Precision: 0.21 ± 0.10\n",
      "  Recall: 0.36 ± 0.06\n",
      "  F1: 0.25 ± 0.09\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.91 ± 0.06\n",
      "  Precision: 0.94 ± 0.05\n",
      "  Recall: 0.92 ± 0.06\n",
      "  F1: 0.92 ± 0.06\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.92 ± 0.05\n",
      "  Precision: 0.95 ± 0.03\n",
      "  Recall: 0.93 ± 0.05\n",
      "  F1: 0.93 ± 0.05\n",
      "\n",
      "=== k = 9 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.05\n",
      "  Precision: 0.98 ± 0.03\n",
      "  Recall: 0.97 ± 0.04\n",
      "  F1: 0.97 ± 0.04\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.90 ± 0.08\n",
      "  Precision: 0.94 ± 0.04\n",
      "  Recall: 0.92 ± 0.07\n",
      "  F1: 0.91 ± 0.08\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.90 ± 0.08\n",
      "  Precision: 0.94 ± 0.04\n",
      "  Recall: 0.92 ± 0.07\n",
      "  F1: 0.91 ± 0.08\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.39 ± 0.12\n",
      "  Precision: 0.22 ± 0.12\n",
      "  Recall: 0.37 ± 0.08\n",
      "  F1: 0.26 ± 0.12\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.09\n",
      "  Precision: 0.78 ± 0.13\n",
      "  Recall: 0.77 ± 0.07\n",
      "  F1: 0.74 ± 0.10\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.39 ± 0.12\n",
      "  Precision: 0.22 ± 0.12\n",
      "  Recall: 0.37 ± 0.08\n",
      "  F1: 0.26 ± 0.12\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.90 ± 0.08\n",
      "  Precision: 0.93 ± 0.06\n",
      "  Recall: 0.91 ± 0.07\n",
      "  F1: 0.91 ± 0.08\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.91 ± 0.12\n",
      "  Precision: 0.91 ± 0.15\n",
      "  Recall: 0.93 ± 0.10\n",
      "  F1: 0.91 ± 0.13\n",
      "\n",
      "=== k = 10 ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.05\n",
      "  Precision: 0.98 ± 0.04\n",
      "  Recall: 0.97 ± 0.04\n",
      "  F1: 0.97 ± 0.04\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.88 ± 0.13\n",
      "  Precision: 0.90 ± 0.15\n",
      "  Recall: 0.90 ± 0.09\n",
      "  F1: 0.88 ± 0.13\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.88 ± 0.13\n",
      "  Precision: 0.90 ± 0.15\n",
      "  Recall: 0.90 ± 0.09\n",
      "  F1: 0.88 ± 0.13\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.33 ± 0.05\n",
      "  Precision: 0.14 ± 0.07\n",
      "  Recall: 0.33 ± 0.05\n",
      "  F1: 0.19 ± 0.06\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.74 ± 0.10\n",
      "  Precision: 0.78 ± 0.12\n",
      "  Recall: 0.77 ± 0.09\n",
      "  F1: 0.74 ± 0.10\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.33 ± 0.05\n",
      "  Precision: 0.14 ± 0.07\n",
      "  Recall: 0.33 ± 0.05\n",
      "  F1: 0.19 ± 0.06\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.92 ± 0.05\n",
      "  Precision: 0.94 ± 0.04\n",
      "  Recall: 0.94 ± 0.04\n",
      "  F1: 0.93 ± 0.05\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.90 ± 0.07\n",
      "  Precision: 0.93 ± 0.06\n",
      "  Recall: 0.91 ± 0.07\n",
      "  F1: 0.91 ± 0.07\n",
      "\n",
      "Best k = 9 with Random Forest (f1 = 0.9706)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 6: Compare Classifiers\n",
    "# This portion of the program uses K-Fold Cross-Validation to perform an initial test phase on each model.\n",
    "# --------------------------\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"AdaBoost\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM (RBF)\": SVC(kernel='rbf'),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"=== Comparing classifiers using Stratified k-Fold Cross-Validation (kFCV)... ===\")\n",
    "\n",
    "best_k = None\n",
    "best_score = -np.inf\n",
    "k_range = range(3, 11)\n",
    "scoring_metric = 'f1'\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"\\n=== k = {k} ===\")\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in classifiers.items():\n",
    "        # results = cross_validate(model, X_train_smote, y_train_smote, cv=skf, scoring=scoring)\n",
    "        results = cross_validate(model, X_train, y_train, cv=skf, scoring=scoring)\n",
    "        avg_score = results[f'test_{scoring_metric}'].mean()\n",
    "        std_score = results[f'test_{scoring_metric}'].std()\n",
    "\n",
    "        print(f\"\\n{name}\")\n",
    "        for metric in scoring:\n",
    "            mean = results[f'test_{metric}'].mean()\n",
    "            std = results[f'test_{metric}'].std()\n",
    "            print(f\"  {metric.capitalize()}: {mean:.2f} ± {std:.2f}\")\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_k = k\n",
    "            best_model_name = name\n",
    "\n",
    "print(f\"\\nBest k = {best_k} with {best_model_name} ({scoring_metric} = {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302a9598-1b9d-4c99-aa86-8bc8f8202075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Final Evaluation on Hold-out Test Set using Random Forest ***\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.97        31\n",
      "   macro avg       0.97      0.97      0.97        31\n",
      "weighted avg       0.97      0.97      0.97        31\n",
      "\n",
      "\n",
      "*** Confusion Matrix on Hold-out Test Set using Random Forest ***\n",
      "\n",
      "\t\tPredicted\n",
      "\t\t0\t1\t2\n",
      "Actual 0 |\t7\t0\t0\n",
      "Actual 1 |\t0\t11\t1\n",
      "Actual 2 |\t0\t0\t12\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 7: Final Evaluation on Test Set\n",
    "# This portion of the program evaluates the RandomForestClassifier on the dataset and generates a confusion matrix for visualization.\n",
    "# --------------------------\n",
    "best_model = classifiers[best_model_name]\n",
    "# best_model.fit(X_train_smote, y_train_smote)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"\\n*** Final Evaluation on Hold-out Test Set using {best_model_name} ***\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(f\"\\n*** Confusion Matrix on Hold-out Test Set using {best_model_name} ***\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n\\t\\tPredicted\\n\\t\\t0\\t1\\t2\")\n",
    "for i, row in enumerate(cm):\n",
    "    print(f\"Actual {i} |\\t\" + \"\\t\".join(str(val) for val in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766f3160-13b9-4a42-bc46-3843df20c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compressing Random Forest for PYNQ Deployment... ===\n",
      "\n",
      "Added 'trusthub_bitstreams' to archive.\n",
      "Added 'model_components' to archive.\n",
      "Added 'VirtualEnv' to archive.\n",
      "Added 'deploy_model.ipynb' to archive.\n",
      "Added 'requirements.txt' to archive.\n",
      "\n",
      "Compression complete. Archive saved as 'pynq_maldetect.tar.gz'.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 8: Export the Trained Model for Deployment\n",
    "# This portion of the program exports the trained RandomForestClassifier for deployment in a target embedded environment (PYNQ-Z1).\n",
    "# --------------------------\n",
    "dump(best_model, './model_components/random_forest_model.joblib')\n",
    "dump(tsvd, './model_components/tsvd.joblib')\n",
    "\n",
    "tsvd = load(\"./model_components/tsvd.joblib\")\n",
    "np.save(\"./model_components/tsvd_components.npy\", tsvd.components_)\n",
    "\n",
    "rf = load(\"./model_components/random_forest_model.joblib\")\n",
    "\n",
    "def extract_tree(tree):\n",
    "    return {\n",
    "        \"children_left\": tree.children_left.tolist(),\n",
    "        \"children_right\": tree.children_right.tolist(),\n",
    "        \"feature\": tree.feature.tolist(),\n",
    "        \"threshold\": tree.threshold.tolist(),\n",
    "        \"value\": tree.value.squeeze(1).tolist()\n",
    "    }\n",
    "\n",
    "forest_json = [extract_tree(estimator.tree_) for estimator in rf.estimators_]\n",
    "\n",
    "with open(\"./model_components/rf_forest.json\", \"w\") as f:\n",
    "    json.dump(forest_json, f)\n",
    "\n",
    "def compress_to_tar_gz(output_filename, items_to_compress):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        for item in items_to_compress:\n",
    "            if os.path.exists(item):\n",
    "                tar.add(item, arcname=os.path.basename(item))\n",
    "                print(f\"Added '{item}' to archive.\")\n",
    "            else:\n",
    "                print(f\"Warning: '{item}' not found and will be skipped.\")\n",
    "\n",
    "print(f\"=== Compressing {best_model_name} for PYNQ Deployment... ===\\n\")\n",
    "\n",
    "targets = [\"trusthub_bitstreams\", \"model_components\", \"VirtualEnv\", \"deploy_model.ipynb\", \"requirements.txt\"]\n",
    "output_file = \"pynq_maldetect.tar.gz\"\n",
    "\n",
    "compress_to_tar_gz(output_file, targets)\n",
    "print(f\"\\nCompression complete. Archive saved as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caee983-e87c-48e6-86c9-73489b7aa763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
