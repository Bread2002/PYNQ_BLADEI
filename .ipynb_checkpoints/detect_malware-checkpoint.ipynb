{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dee91d9-660d-42b2-9be1-eaef72e680a3",
   "metadata": {},
   "source": [
    "\n",
    "# **Machine Learning-Based Detection of Simulated Malware in FPGA Bitstreams**\n",
    "### *Copyright (c) 2025, Rye Stahle-Smith* \n",
    "---\n",
    "#### ***Description:***\n",
    "This project explores the application of machine learning techniques to detect malicious modifications in FPGA bitstreams, specifically targeting the *PYNQ-Z1 FPGA Development Board* by Xilinx and Digilent. Rather than relying on custom-generated data, the project now leverages state-of-the-art (SOTA) benchmarks from **Trust-Hub**, a resource sponsored by the **National Science Foundation (NSF)**. Although these benchmarks have not been updated since 2013, they represent realistic chip-level attack scenarios. The original designs were re-engineered and adapted to work with the PYNQ-Z1 platform. As of the latest update, the dataset consists of ***one-hundred and twenty-two samples*** — ***twenty-five benign AES-128, twenty-one benign RS232, twenty-five benign AES-128, twenty-one benign RS232, and thirty empty bitstreams***.\n",
    "\n",
    "#### ***Objectives:***\n",
    "**Objective 1: Integrate and adapt industry-standard benchmarks for modern FPGA platforms.** <br>\n",
    "Task 1.1: Reconfigure Trust-Hub HDL benchmarks to compile and synthesize successfully using Xilinx Vivado for compatibility with PYNQ-Z1. <br>\n",
    "Task 1.2: Synthesize and implement modules on the PYNQ-Z1 board to verify functional correctness and hardware compatibility. <br>\n",
    "Task 1.3: Organize, label, and curate the bitstream dataset across multiple classes (benign, malicious, empty) for use in supervised learning. <br>\n",
    "\n",
    "**Objective 2: Design an ML-based detection pipeline for hardware Trojans in FPGA bitstreams.** <br>\n",
    "Task 2.1: Convert raw .bit files into sparse byte-frequency representations and apply dimensionality reduction using Truncated Singular Value Decomposition (TSVD). <br>\n",
    "Task 2.2: Apply SMOTE (Synthetic Minority Oversampling Technique) to balance class distributions in the training set and mitigate bias. <br>\n",
    "Task 2.3: Train and compare multiple classifiers (Random Forest, Decision Tree, SVM, Gradient Boosting, etc.) using k-fold cross-validation to identify the most effective model. <br>\n",
    "\n",
    "**Objective 3: Evaluate model performance and ensure robustness across Trojan variants.** <br>\n",
    "Task 3.1: Analyze performance using metrics such as accuracy, precision, recall, F1-score, and class-wise true/false positive rates. <br>\n",
    "Task 3.2: Generate confusion matrices and perform error analysis to assess model weaknesses and edge cases. <br>\n",
    "Task 3.3: Assess model robustness by deploying the trained model on the PYNQ-Z1 platform to validate real-time prediction performance and resource efficiency in a target embedded environment. <br>\n",
    "\n",
    "#### ***Sources:***\n",
    "Hayashi, V. T., & Vicente Ruggiero, W. (2025). Hardware Trojan Detection in Open-Source Hardware Designs Using Machine Learning. *IEEE Access*. https://ieeexplore.ieee.org/document/10904479 <br>\n",
    "Elnaggar, R., Chaudhuri, J., Karri, R., & Chakrabarty, K. (2023). Learning Malicious Circuits in FPGA Bitstreams. *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*, 42(3), 726–739. https://ieeexplore.ieee.org/document/9828544<br>\n",
    "Pedregosa, F., et al. (2011). scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research*, 12, 2825–2830. https://dl.acm.org/doi/10.5555/1953048.2078195<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2ae3b4-db46-4960-b660-abb8ce8c6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Organizing bitstreams... ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "from joblib import dump\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Collect Bitstreams\n",
    "# This portion of the program organizes the bitstreams respectively.\n",
    "# --------------------------\n",
    "EMPTY_FILES = glob.glob(\"trusthub_bitstreams/Empty/*.bit\")\n",
    "BENIGN_AES_FILES = glob.glob(\"trusthub_bitstreams/Benign/AES*.bit\")\n",
    "BENIGN_RS232_FILES = glob.glob(\"trusthub_bitstreams/Benign/RS232*.bit\")\n",
    "MAL_AES_FILES = glob.glob(\"trusthub_bitstreams/Malicious/AES*.bit\")\n",
    "MAL_RS232_FILES = glob.glob(\"trusthub_bitstreams/Malicious/RS232*.bit\")\n",
    "ALL_FILES = EMPTY_FILES + BENIGN_AES_FILES + BENIGN_RS232_FILES + MAL_AES_FILES + MAL_RS232_FILES\n",
    "print(\"=== Organizing bitstreams... ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d14e1cc-de95-4c63-b437-005de45011b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extracting sparse features... ===\n",
      "Progress: |████████████████████| 100% (122/122)\n",
      "=== Defining labels... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 2: Feature Extraction\n",
    "# This portion of the program sets the features and labels for the dataset.\n",
    "# --------------------------\n",
    "def extract_sparse_features(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = f.read()\n",
    "    size = len(data)\n",
    "    if size == 0:\n",
    "        return np.zeros(256)\n",
    "    counts = Counter(data)\n",
    "    dense_vec = np.zeros(256)\n",
    "    for byte_val, count in counts.items():\n",
    "        dense_vec[byte_val] = count / size\n",
    "    return dense_vec\n",
    "\n",
    "def display_progress(current, total):\n",
    "    bar_length = 20\n",
    "    percent = int((current / total) * 100)\n",
    "    blocks = int((current / total) * bar_length)\n",
    "    bar = '█' * blocks + '-' * (bar_length - blocks)\n",
    "    sys.stdout.write(f'\\rProgress: |{bar}| {percent}% ({current}/{total})')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"=== Extracting sparse features... ===\")\n",
    "feature_matrix = []\n",
    "for i, f in enumerate(ALL_FILES, 1):\n",
    "    feature_matrix.append(extract_sparse_features(f))\n",
    "    display_progress(i, len(ALL_FILES))\n",
    "\n",
    "print(\"\\n=== Defining labels... ===\")\n",
    "y = [0]*len(EMPTY_FILES) + [1]*len(BENIGN_AES_FILES) + [2]*len(BENIGN_RS232_FILES) + [3]*len(MAL_AES_FILES) + [4]*len(MAL_RS232_FILES)\n",
    "X = np.array(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0118a8e-238d-45d4-a23f-f4eba01f8c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Applying Truncated Singular Value Decomposition (TSVD)... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 3: Apply Truncated Singular Value Decomposition (TSVD)\n",
    "# This portion of the program uses TSVD to reduce dataset dimensionality and preserve its most important characteristics.\n",
    "# --------------------------\n",
    "print(\"=== Applying Truncated Singular Value Decomposition (TSVD)... ===\")\n",
    "sparse_X = csr_matrix(X)\n",
    "tsvd = TruncatedSVD(n_components=30, random_state=42)\n",
    "X_reduced = tsvd.fit_transform(sparse_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c6df8e-4a19-4103-a6f1-5363878f2fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Splitting the dataset for training/ testing... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 4: Train/Test Split\n",
    "# This portion of the program splits and scales the dataset into a complete training and testing set.\n",
    "# --------------------------\n",
    "print(\"=== Splitting the dataset for training/ testing... ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c88b80-5c3d-45d7-bb6a-0d221a948325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparing k_neighbors values for Synthetic Minority Oversampling Technique (SMOTE)... ===\n",
      "\n",
      "SMOTE (k=2): F1 Macro = 0.9820 ± 0.0360\n",
      "SMOTE (k=5): F1 Macro = 0.9810 ± 0.0234\n",
      "SMOTE (k=7): F1 Macro = 0.9830 ± 0.0208\n",
      "SMOTE (k=9): F1 Macro = 0.9729 ± 0.0224\n",
      "SMOTE (k=11): F1 Macro = 0.9650 ± 0.0334\n",
      "\n",
      "=== Applying SMOTE with k_neighbors=7... ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 5: Apply SMOTE on Training Set\n",
    "# This portion of the program uses SMOTE to balance class distributions by generating synthetic samples for minority classes.\n",
    "# Thus, improving model performance on imbalanced datasets.\n",
    "# --------------------------\n",
    "print(\"=== Comparing k_neighbors values for Synthetic Minority Oversampling Technique (SMOTE)... ===\\n\")\n",
    "k_values = [2, 5, 7, 9, 11]\n",
    "smote_results = {}\n",
    "best_k = None\n",
    "best_score = 0\n",
    "\n",
    "for k in k_values:\n",
    "    try:\n",
    "        smote = SMOTE(k_neighbors=k, random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        scores = cross_val_score(model, X_train_smote, y_train_smote, cv=5, scoring='f1_macro')\n",
    "        mean_score = scores.mean()\n",
    "        \n",
    "        smote_results[k] = mean_score\n",
    "        print(f\"SMOTE (k={k}): F1 Macro = {mean_score:.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"SMOTE (k={k}) failed: {e}\")\n",
    "\n",
    "print(f\"\\n=== Applying SMOTE with k_neighbors={best_k}... ===\")\n",
    "smote = SMOTE(k_neighbors=best_k, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a899c7-4b1e-4c99-9937-c68dda3c6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparing classifiers using k-Fold Cross-Validation (kFCV)... ===\n",
      "\n",
      "Random Forest\n",
      "  Accuracy: 0.97 ± 0.04\n",
      "  Precision: 0.98 ± 0.04\n",
      "  Recall: 0.97 ± 0.04\n",
      "  F1: 0.97 ± 0.04\n",
      "\n",
      "Gradient Boosting\n",
      "  Accuracy: 0.90 ± 0.05\n",
      "  Precision: 0.88 ± 0.11\n",
      "  Recall: 0.90 ± 0.06\n",
      "  F1: 0.88 ± 0.09\n",
      "\n",
      "AdaBoost\n",
      "  Accuracy: 0.90 ± 0.05\n",
      "  Precision: 0.88 ± 0.11\n",
      "  Recall: 0.90 ± 0.06\n",
      "  F1: 0.88 ± 0.09\n",
      "\n",
      "Logistic Regression\n",
      "  Accuracy: 0.33 ± 0.07\n",
      "  Precision: 0.14 ± 0.03\n",
      "  Recall: 0.36 ± 0.08\n",
      "  F1: 0.20 ± 0.05\n",
      "\n",
      "Naive Bayes\n",
      "  Accuracy: 0.95 ± 0.06\n",
      "  Precision: 0.96 ± 0.05\n",
      "  Recall: 0.96 ± 0.05\n",
      "  F1: 0.96 ± 0.05\n",
      "\n",
      "SVM (RBF)\n",
      "  Accuracy: 0.46 ± 0.10\n",
      "  Precision: 0.30 ± 0.13\n",
      "  Recall: 0.51 ± 0.11\n",
      "  F1: 0.36 ± 0.12\n",
      "\n",
      "KNN\n",
      "  Accuracy: 0.86 ± 0.04\n",
      "  Precision: 0.87 ± 0.04\n",
      "  Recall: 0.86 ± 0.03\n",
      "  F1: 0.86 ± 0.04\n",
      "\n",
      "Decision Tree\n",
      "  Accuracy: 0.94 ± 0.02\n",
      "  Precision: 0.94 ± 0.03\n",
      "  Recall: 0.94 ± 0.02\n",
      "  F1: 0.94 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 6: Compare Classifiers\n",
    "# This portion of the program uses K-Fold Cross-Validation to perform an initial test phase on each model.\n",
    "# --------------------------\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"AdaBoost\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"SVM (RBF)\": SVC(kernel='rbf'),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"=== Comparing classifiers using k-Fold Cross-Validation (kFCV)... ===\")\n",
    "cv_results = {}\n",
    "k = 5\n",
    "for name, model in classifiers.items():\n",
    "    results = cross_validate(model, X_train_smote, y_train_smote, cv=k, scoring=scoring)\n",
    "    print(f\"\\n{name}\")\n",
    "    for metric in scoring:\n",
    "        mean = results[f'test_{metric}'].mean()\n",
    "        std = results[f'test_{metric}'].std()\n",
    "        print(f\"  {metric.capitalize()}: {mean:.2f} ± {std:.2f}\")\n",
    "    cv_results[name] = results['test_f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302a9598-1b9d-4c99-aa86-8bc8f8202075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Final Evaluation on Hold-out Test Set using Random Forest ***\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       0.86      1.00      0.92         6\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      0.86      0.92         7\n",
      "           4       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.97        31\n",
      "   macro avg       0.97      0.97      0.97        31\n",
      "weighted avg       0.97      0.97      0.97        31\n",
      "\n",
      "\n",
      "*** Confusion Matrix on Hold-out Test Set using Random Forest ***\n",
      "\n",
      "\t\tPredicted\n",
      "\t\t0\t1\t2\t3\t4\n",
      "Actual 0 |\t8\t0\t0\t0\t0\n",
      "Actual 1 |\t0\t6\t0\t0\t0\n",
      "Actual 2 |\t0\t0\t5\t0\t0\n",
      "Actual 3 |\t0\t1\t0\t6\t0\n",
      "Actual 4 |\t0\t0\t0\t0\t5\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 7: Final Evaluation on Test Set\n",
    "# This portion of the program evaluates the RandomForestClassifier on the dataset and generates a confusion matrix for visualization.\n",
    "# --------------------------\n",
    "best_model_name = max(cv_results, key=cv_results.get)\n",
    "best_model = classifiers[best_model_name]\n",
    "best_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"\\n*** Final Evaluation on Hold-out Test Set using {best_model_name} ***\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(f\"\\n*** Confusion Matrix on Hold-out Test Set using {best_model_name} ***\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n\\t\\tPredicted\\n\\t\\t0\\t1\\t2\\t3\\t4\")\n",
    "for i, row in enumerate(cm):\n",
    "    print(f\"Actual {i} |\\t\" + \"\\t\".join(str(val) for val in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766f3160-13b9-4a42-bc46-3843df20c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tsvd.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Step 8: Export the Trained Model for Deployment\n",
    "# This portion of the program exports the trained RandomForestClassifier for deployment in a target embedded environment (PYNQ-Z1).\n",
    "# --------------------------\n",
    "dump(best_model, 'random_forest_model.joblib')\n",
    "dump(tsvd, 'tsvd.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
